{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('ST.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "SR.No        0\n",
      "text         0\n",
      "Timestamp    0\n",
      "Username     0\n",
      "Platform     0\n",
      "Hashtags     0\n",
      "Retweets     0\n",
      "Likes        0\n",
      "Country      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count missing values per column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Display missing values per column\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+', '', str(text))  # Remove URLs\n",
    "    text = re.sub(r'@\\w+|\\#', '', str(text))   # Remove mentions and hashtags\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')  # Remove emojis\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = word_tokenize(text)  # Tokenize\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords\n",
    "    cleaned_text = ' '.join(tokens)  # Join tokens\n",
    "    return cleaned_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply text preprocessing\n",
    "df['cleaned_text'] = df['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     SR.No                                               text  \\\n",
      "0        1   Enjoying a beautiful day at the park!        ...   \n",
      "1        2   Traffic was terrible this morning.           ...   \n",
      "2        3   Just finished an amazing workout! ðŸ’ª          ...   \n",
      "3        4   Excited about the upcoming weekend getaway!  ...   \n",
      "4        5   Trying out a new recipe for dinner tonight.  ...   \n",
      "..     ...                                                ...   \n",
      "727    729  Collaborating on a science project that receiv...   \n",
      "728    730  Attending a surprise birthday party organized ...   \n",
      "729    731  Successfully fundraising for a school charity ...   \n",
      "730    732  Participating in a multicultural festival, cel...   \n",
      "731    733  Organizing a virtual talent show during challe...   \n",
      "\n",
      "            Timestamp                               Username     Platform  \\\n",
      "0    15-01-2023 12:30                          User123          Twitter     \n",
      "1    15-01-2023 08:45                          CommuterX        Twitter     \n",
      "2    15-01-2023 15:45                          FitnessFan      Instagram    \n",
      "3    15-01-2023 18:20                          AdventureX       Facebook    \n",
      "4    15-01-2023 19:55                          ChefCook        Instagram    \n",
      "..                ...                                    ...          ...   \n",
      "727  18-08-2017 18:20       ScienceProjectSuccessHighSchool     Facebook    \n",
      "728  22-06-2018 14:15            BirthdayPartyJoyHighSchool    Instagram    \n",
      "729  05-04-2019 17:30   CharityFundraisingTriumphHighSchool      Twitter    \n",
      "730  29-02-2020 20:45    MulticulturalFestivalJoyHighSchool     Facebook    \n",
      "731  15-11-2020 15:15    VirtualTalentShowSuccessHighSchool    Instagram    \n",
      "\n",
      "                                          Hashtags  Retweets  Likes  \\\n",
      "0        #Nature #Park                                    15     30   \n",
      "1        #Traffic #Morning                                 5     10   \n",
      "2        #Fitness #Workout                                20     40   \n",
      "3        #Travel #Adventure                                8     15   \n",
      "4        #Cooking #Food                                   12     25   \n",
      "..                                             ...       ...    ...   \n",
      "727         #ScienceFairWinner #HighSchoolScience         20     39   \n",
      "728    #SurpriseCelebration #HighSchoolFriendship         25     48   \n",
      "729      #CommunityGiving #HighSchoolPhilanthropy         22     42   \n",
      "730         #CulturalCelebration #HighSchoolUnity         21     43   \n",
      "731   #VirtualEntertainment #HighSchoolPositivity         24     47   \n",
      "\n",
      "          Country                                       cleaned_text  \\\n",
      "0       USA                            enjoying beautiful day park !   \n",
      "1       Canada                            traffic terrible morning .   \n",
      "2     USA                                 finished amazing workout !   \n",
      "3       UK                        excited upcoming weekend getaway !   \n",
      "4      Australia                  trying new recipe dinner tonight .   \n",
      "..            ...                                                ...   \n",
      "727            UK  collaborating science project received recogni...   \n",
      "728           USA  attending surprise birthday party organized fr...   \n",
      "729        Canada  successfully fundraising school charity initia...   \n",
      "730            UK  participating multicultural festival , celebra...   \n",
      "731           USA  organizing virtual talent show challenging tim...   \n",
      "\n",
      "     topic_1_prob  topic_2_prob  \n",
      "0        0.206901      0.793099  \n",
      "1        0.195604      0.804396  \n",
      "2        0.192795      0.807205  \n",
      "3        0.828070      0.171930  \n",
      "4        0.837250      0.162750  \n",
      "..            ...           ...  \n",
      "727      0.149470      0.850530  \n",
      "728      0.766597      0.233403  \n",
      "729      0.853705      0.146295  \n",
      "730      0.862298      0.137702  \n",
      "731      0.262134      0.737866  \n",
      "\n",
      "[732 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA) for topic modeling\n",
    "lda_model = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "# Add sentiment score and LDA topic features to dataframe\n",
    "df['topic_1_prob'] = lda_matrix[:, 0]\n",
    "df['topic_2_prob'] = lda_matrix[:, 1]\n",
    "\n",
    "# Display the dataframe with engineered features\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
